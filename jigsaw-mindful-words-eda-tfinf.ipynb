{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<h1 id=\"heading\">\n1. ‚ò†Ô∏é Background Story\n<a class=\"anchor-link\" \n href=\"https://www.kaggle.com/fangya/jigsaw-mindful-words-eda-tfinf\">¬∂</a>\n</h1>\n\nIn January 2022, while the teenager Xuezhou Liu (ÂàòÂ≠¶Â∑û) was trying to find his birth parents through social media, he was attacked by malicious comments and cyber bullying. He commited suicide and died at the age of 15.  \n\n<div class=\"alert alert-block alert-info\"> üìå Be mindful of what we say because our comments actually Matters !</div>\n\n<img src=\"https://p7.itc.cn/images01/20220124/0f138c3cf26644f9a2bde175473bda6f.png\"  width=\"440\" align=\"center\">","metadata":{}},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom PIL import Image\nimport json\nfrom tqdm import tqdm\ntqdm.pandas()\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport os\nimport gc\nfrom textblob import TextBlob\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\n\nimport gensim\nfrom sklearn.model_selection import KFold\n\nfrom IPython.display import SVG\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('ggplot')\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn import svm\nfrom scipy.sparse import hstack\nfrom collections import defaultdict\nimport plotly.graph_objects as gobs\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import MultinomialNB\n\nfrom tqdm.auto import tqdm\nfrom bs4 import BeautifulSoup\nfrom collections import defaultdict\nimport re \nimport scipy\nfrom scipy import sparse\nfrom IPython.display import display\nfrom pprint import pprint\nfrom matplotlib import pyplot as plt \nimport time\nimport scipy.optimize as optimize\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.linear_model import Ridge\nimport zipfile\nimport string\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nimport string\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words(\"english\")) \nlemmatizer = WordNetLemmatizer() \nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:12:24.12932Z","iopub.execute_input":"2022-02-03T20:12:24.130257Z","iopub.status.idle":"2022-02-03T20:13:26.806015Z","shell.execute_reply.started":"2022-02-03T20:12:24.1302Z","shell.execute_reply":"2022-02-03T20:13:26.805079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"heading\">\n2. üìù Introduction\n<a class=\"anchor-link\" \nhref=\"https://www.kaggle.com/fangya/jigsaw-mindful-words-eda-tfinf\">¬∂</a>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"#### Motivations:\n\nFrom a marketing persepective, NLP can help companies efficiently classify the customer reviews and bulid accurate algorithm for merchdise rating to better serve future customers.  Furthermore, extracting out toxic comments by its toxicity levels and curb baleful actions in time could mitigate cyber-violence and personal attacks, and building up a harmonious virtrul virtual. Besides, it could also detect and prevent potential crimes by these key words which might tire up malignant and aggresive behaviors in the real world. \n\nIf the malicious comments was blocked by the social media toxic comment algorithm and didn't flood in XueZhou's inbox, maybe he can celebrate the Year of Tiger!\n\n\n#### Project Outline:\n\n1. Text Cleaning\n\n2. Data Visualization\n  - EDA\n  - WordCloud\n\n\n3. Modeling\n  - TF-IDF \n  - Model Compairson\n  \n#### Team Members:\n@Eileen_ds @Kathleen Lee @Kehan Liu @Xiaoke Yu","metadata":{}},{"cell_type":"markdown","source":"<h1 id=\"heading\">\n3. üßπ Text Cleaning\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/fangya/jigsaw-mindful-words-eda-tfinf\">¬∂</a>\n</h1>","metadata":{}},{"cell_type":"markdown","source":"**Clean Text** is human language rearranged into a format that machine models can understand. \n\n> If the Text is Clean, it doesn't matter which model we run \n>                                              -Eileen+CQL \n\nWe will perform the below steps to clean the data\n1. Remove Special Characters\n   - unicode, *\n   \n   \n2. Normalize the text cases \n   - lower, upper, proper\n\n\n3. Remove puncations\n   - !, .? @_@\n\n\n4. Remove Stop words\n   - wikipedia\n\n\n5. Tokenization\n   - Unigram, Bi-grams, Tri-grams\n   \n   I Love you\n   \\\n   Unigram: I, Love, You\n   \\\n   Bi-gram: I Love, Love You\n   \\\n   Tri-gram: I Love You\n\n\n6. Stemming vs Lemmatization\n   - root definition: Jump ,Jumps, Jumped\n   - root definition with tense : Jumped vs Jump","metadata":{}},{"cell_type":"markdown","source":"#### Load Dataset\n1. Jigsaw Toxic Severity rating\n2. Jigsaw Toxic Comment classfication","metadata":{}},{"cell_type":"code","source":"# Load dataset\nval= pd.read_csv('../input/jigsaw-toxic-severity-rating/validation_data.csv')\ndf_train= pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ndf_train1= pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/train.csv')\ntrain_csv_path = './train.csv'\nsample_sub_path = '../input/jigsaw-toxic-severity-rating/sample_submission.csv'\ncomments_to_score_path = '../input/jigsaw-toxic-severity-rating/comments_to_score.csv'\ndf_test=pd.read_csv('../input/jigsaw-toxic-comment-classification-challenge/test.csv')\ndf_sub = pd.read_csv(\"../input/jigsaw-toxic-severity-rating/comments_to_score.csv\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:45:18.513598Z","iopub.execute_input":"2022-02-03T19:45:18.513881Z","iopub.status.idle":"2022-02-03T19:45:22.201945Z","shell.execute_reply.started":"2022-02-03T19:45:18.513853Z","shell.execute_reply":"2022-02-03T19:45:22.201002Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:45:25.77118Z","iopub.execute_input":"2022-02-03T19:45:25.771503Z","iopub.status.idle":"2022-02-03T19:45:25.782814Z","shell.execute_reply.started":"2022-02-03T19:45:25.771474Z","shell.execute_reply":"2022-02-03T19:45:25.781879Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:35:05.892284Z","iopub.execute_input":"2022-02-03T19:35:05.893112Z","iopub.status.idle":"2022-02-03T19:35:05.909125Z","shell.execute_reply.started":"2022-02-03T19:35:05.893058Z","shell.execute_reply":"2022-02-03T19:35:05.908115Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Data Processing","metadata":{}},{"cell_type":"code","source":"# Clean Text\ndef clean_text(text):\n#replace the html characters with \" \"\n    text=re.sub('<.*?>', ' ', text)  \n#remove the punctuations\n    text = text.translate(str.maketrans(' ',' ',string.punctuation))\n#consider only alphabets and numerics\n    text = re.sub('[^a-zA-Z]',' ',text)  \n#replace newline with space\n    text = re.sub(\"\\n\",\" \",text)\n#convert to lower case\n    text = text.lower()\n#split and join the words\n    text=' '.join(text.split())\n    return text\n\ndef stopwords(input_text, stop_words):\n    word_tokens = word_tokenize(input_text) \n    output_text = [w for w in word_tokens if not w in stop_words]\n    output = [] \n    for w in word_tokens: \n        if w not in stop_words:\n            output.append(w)\n            \n    text = ' '.join(output)\n    return text","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:45:29.220103Z","iopub.execute_input":"2022-02-03T19:45:29.22039Z","iopub.status.idle":"2022-02-03T19:45:29.228511Z","shell.execute_reply.started":"2022-02-03T19:45:29.220363Z","shell.execute_reply":"2022-02-03T19:45:29.227545Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unrelevant_words = ['wiki','wikipedia','page']\ndef clean(data,word):\n    #Clean step 1, 2 and 3\n    data[word] = data[word].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n    #Clean Step 4\n    data[word] = data[word].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n    #Clean Step 5\n    data[word] = data[word].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n  ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:45:32.768529Z","iopub.execute_input":"2022-02-03T19:45:32.768806Z","iopub.status.idle":"2022-02-03T19:45:32.776541Z","shell.execute_reply.started":"2022-02-03T19:45:32.768781Z","shell.execute_reply":"2022-02-03T19:45:32.775551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Clean Comment Text\nclean(df_train,\"comment_text\")\ndf_train.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:45:37.12476Z","iopub.execute_input":"2022-02-03T19:45:37.125454Z","iopub.status.idle":"2022-02-03T19:47:44.15735Z","shell.execute_reply.started":"2022-02-03T19:45:37.125418Z","shell.execute_reply":"2022-02-03T19:47:44.156425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Clean validation text\nclean(val,\"less_toxic\")\nclean(val,\"more_toxic\")\nval.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:37:15.33632Z","iopub.execute_input":"2022-02-03T19:37:15.336563Z","iopub.status.idle":"2022-02-03T19:38:10.713729Z","shell.execute_reply.started":"2022-02-03T19:37:15.336535Z","shell.execute_reply":"2022-02-03T19:38:10.712977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"heading\">\n4. üß© Data Visualization\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/fangya/jigsaw-mindful-words/edit/run/84913255\">¬∂</a>\n</h1>\n\nWe will present basic Frequency count, bar plot and WordCloud for readers to better interpret and understanding the toxic comments. ","metadata":{}},{"cell_type":"markdown","source":"<h2 id=\"heading\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n<p style=\"padding: 5px;\n              color:white;\">\n           EDA\n</p>\n</div>\n\n</h2>","metadata":{}},{"cell_type":"markdown","source":"#### Worker Load\nWe can see most workers process less than 50 rows, we think this diveristy of worker will minimize the reviewers bias.","metadata":{}},{"cell_type":"code","source":"w = val['worker'].value_counts() \\\n    .plot(kind='hist', bins=50,\n          color=\"orchid\", figsize=(12, 5))\nw.set_title('Frequeny of Worker in Val Set', fontsize=20)\nw.set_xlabel('Rows in Validation set for a Worker')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:10.714924Z","iopub.execute_input":"2022-02-03T19:38:10.715162Z","iopub.status.idle":"2022-02-03T19:38:11.12803Z","shell.execute_reply.started":"2022-02-03T19:38:10.71512Z","shell.execute_reply":"2022-02-03T19:38:11.127149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Less vs More Toxic\n\nWe can see More Toxic Comments has less words in count, most of extremely angry people don't have a lot of say, or they don't have many vocabulory in mind.","metadata":{}},{"cell_type":"code","source":"text1 = 'less_toxic'\ntext2 = \"more_toxic\"\n#Number of characters\ndef lplot(data, var, color, label):\n    fig, ax=plt.subplots(1,1)\n    length=data[var].apply(len)\n    data[\"length\"]=length\n    length = data.loc[data[\"length\"]<1000][\"length\"]\n    sns.distplot(length, color=color)\n    plt.suptitle(label)\n    return plt\n\nlplot(val,text1,\"g\", \"Less Toxic Comments Length\")\nlplot(val, text2,\"red\", \"More Toxic Comments Length\")\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:11.129725Z","iopub.execute_input":"2022-02-03T19:38:11.13005Z","iopub.status.idle":"2022-02-03T19:38:12.242053Z","shell.execute_reply.started":"2022-02-03T19:38:11.130012Z","shell.execute_reply":"2022-02-03T19:38:12.241015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Number of words\ndef wplot(data, var,color, label):\n    fig, ax=plt.subplots(1,1)\n    words=val[var].apply(lambda x: len(x) - len(''.join(x.split())) + 1)\n    val['words'] = words\n    words = val.loc[val['words']<200]['words']\n    sns.distplot(words, color=color)\n    plt.suptitle(label)\n    return plt\n\nwplot(val,text1,\"g\", \"Less Toxic Comments Words\")\nwplot(val, text2,\"red\", \"More Toxic Comments Words\")\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:12.243399Z","iopub.execute_input":"2022-02-03T19:38:12.243654Z","iopub.status.idle":"2022-02-03T19:38:13.534985Z","shell.execute_reply.started":"2022-02-03T19:38:12.243626Z","shell.execute_reply":"2022-02-03T19:38:13.534236Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Average word length\ndef awdplt(data, var, color,label):\n    fig, ax=plt.subplots(1,1)\n    avg_word_len = data[var].apply(lambda x: 1.0*len(''.join(x.split()))/(len(x) - len(''.join(x.split())) + 1))\n    val['avg_word_len'] = avg_word_len\n    avg_word_len = val.loc[data['avg_word_len']<10]['avg_word_len']\n    sns.distplot(avg_word_len, color=color)\n    plt.suptitle(label)\n    return plt\n\nawdplt(val,text1,\"skyblue\", \"Less Toxic Comments Average Words Length\")\nawdplt(val, text2,\"b\", \"More Toxic Comments Average Words Length\")\nplt.show() ","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:13.536351Z","iopub.execute_input":"2022-02-03T19:38:13.53723Z","iopub.status.idle":"2022-02-03T19:38:15.000689Z","shell.execute_reply.started":"2022-02-03T19:38:13.537188Z","shell.execute_reply":"2022-02-03T19:38:14.999913Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_toxic_words = [len(sentence.split(' ')) for sentence in val['less_toxic'].values]\nmore_toxic_words = [len(sentence.split(' ')) for sentence in val['more_toxic'].values]\n\n\nfig = gobs.Figure()\nfig.add_trace(gobs.Box(y=less_toxic_words, name = 'less_toxic',))\nfig.add_trace(gobs.Box(y=more_toxic_words, name = 'more_toxic'))\n\nfig.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:15.004994Z","iopub.execute_input":"2022-02-03T19:38:15.005242Z","iopub.status.idle":"2022-02-03T19:38:15.849874Z","shell.execute_reply.started":"2022-02-03T19:38:15.005216Z","shell.execute_reply":"2022-02-03T19:38:15.848969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Tokenization\nThis is different types of Tokenization,\nWhite-space Tokenization, Regular Expression Tokenization, Sentence Tokenization, and Word Tokenization.\n\nWe will perform Uni-Gram and Bi-Gram tokenization in our example","metadata":{}},{"cell_type":"code","source":"#n-gram generator\ndef ngram(text, n_gram=1):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:15.850993Z","iopub.execute_input":"2022-02-03T19:38:15.851216Z","iopub.status.idle":"2022-02-03T19:38:15.856846Z","shell.execute_reply.started":"2022-02-03T19:38:15.851191Z","shell.execute_reply":"2022-02-03T19:38:15.85611Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 18\nless_toxic_unigrams = defaultdict(int)\nfor tweet in val['less_toxic']:\n    for word in ngram(tweet, 1):\n        less_toxic_unigrams[word] += 1\n\ndf_less_toxic_unigrams = pd.DataFrame(sorted(less_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\n\nunigrams_less_100 = df_less_toxic_unigrams[:N]\n\nmore_toxic_unigrams = defaultdict(int)\nfor tweet in val['more_toxic']:\n    for word in ngram(tweet, 1):\n        more_toxic_unigrams[word] += 1\n        \ndf_more_toxic_unigrams = pd.DataFrame(sorted(more_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_more_100 = df_more_toxic_unigrams[:N]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:15.858024Z","iopub.execute_input":"2022-02-03T19:38:15.858259Z","iopub.status.idle":"2022-02-03T19:38:17.345231Z","shell.execute_reply.started":"2022-02-03T19:38:15.858233Z","shell.execute_reply":"2022-02-03T19:38:17.344408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"N = 18\nless_toxic_unigrams = defaultdict(int)\nfor tweet in val['less_toxic']:\n    for word in ngram(tweet, 1):\n        less_toxic_unigrams[word] += 1\n\ndf_less_toxic_unigrams = pd.DataFrame(sorted(less_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\n\nunigrams_less_100 = df_less_toxic_unigrams[:N]\n\nmore_toxic_unigrams = defaultdict(int)\nfor tweet in val['more_toxic']:\n    for word in ngram(tweet, 1):\n        more_toxic_unigrams[word] += 1\n        \ndf_more_toxic_unigrams = pd.DataFrame(sorted(more_toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_more_100 = df_more_toxic_unigrams[:N]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:17.346395Z","iopub.execute_input":"2022-02-03T19:38:17.346615Z","iopub.status.idle":"2022-02-03T19:38:18.831902Z","shell.execute_reply.started":"2022-02-03T19:38:17.34659Z","shell.execute_reply":"2022-02-03T19:38:18.831074Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(18, N//2), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=unigrams_less_100[0], x=unigrams_less_100[1], ax=axes[0], color='#97D857')\nsns.barplot(y=unigrams_more_100[0], x=unigrams_more_100[1], ax=axes[1], color='coral')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in less_toxic comments', fontsize=15)\naxes[1].set_title(f'Top {N} most common unigrams in more_toxic comments', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:18.833437Z","iopub.execute_input":"2022-02-03T19:38:18.833749Z","iopub.status.idle":"2022-02-03T19:38:19.568051Z","shell.execute_reply.started":"2022-02-03T19:38:18.83371Z","shell.execute_reply":"2022-02-03T19:38:19.567157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"less_toxic_bigrams = defaultdict(int)\nfor tweet in val['less_toxic']:\n    for word in ngram(tweet, 2):\n        less_toxic_bigrams[word] += 1\n        \ndf_less_toxic_bigrams = pd.DataFrame(sorted(less_toxic_bigrams.items(), key=lambda x: x[1])[::-1])\n\nbigrams_less_100 = df_less_toxic_bigrams[:N]\n\nmore_toxic_bigrams = defaultdict(int)\nfor tweet in val['more_toxic']:\n    for word in ngram(tweet, 2):\n        more_toxic_bigrams[word] += 1\n        \ndf_more_toxic_bigrams = pd.DataFrame(sorted(more_toxic_bigrams.items(), key=lambda x: x[1])[::-1])\n\nbigrams_more_100 = df_more_toxic_bigrams[:N]","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:19.569399Z","iopub.execute_input":"2022-02-03T19:38:19.569782Z","iopub.status.idle":"2022-02-03T19:38:22.149631Z","shell.execute_reply.started":"2022-02-03T19:38:19.569737Z","shell.execute_reply":"2022-02-03T19:38:22.148481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(18, N//2), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=bigrams_less_100[0], x=bigrams_less_100[1], ax=axes[0], color='#b7dcb1')\nsns.barplot(y=bigrams_more_100[0], x=bigrams_more_100[1], ax=axes[1], color='#FF4044')\n\nfor i in range(2):\n    axes[i].spines['right'].set_visible(False)\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common bigrams in less_toxic comments', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in more_toxic comments', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:22.151039Z","iopub.execute_input":"2022-02-03T19:38:22.151315Z","iopub.status.idle":"2022-02-03T19:38:23.1612Z","shell.execute_reply.started":"2022-02-03T19:38:22.151286Z","shell.execute_reply":"2022-02-03T19:38:23.160368Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Training dataset Unigrams + Bigrams\n\nSince we haven't apply sentiment analysis on the training dataset, so the Unigram/Bigram in Comment text doesn't have sentimental approaches, it is simply the most common words/phrases.","metadata":{}},{"cell_type":"code","source":"N = 18\ntoxic_unigrams = defaultdict(int)\nfor tweet in df_train['comment_text']:\n    for word in ngram(tweet, 1):\n        toxic_unigrams[word] += 1\n\ndf_toxic_unigrams = pd.DataFrame(sorted(toxic_unigrams.items(), key=lambda x: x[1])[::-1])\n\nunigrams_100 = df_toxic_unigrams[:N]\n\n\ntoxic_bigrams = defaultdict(int)\nfor tweet in df_train['comment_text']:\n    for word in ngram(tweet, 2):\n        toxic_bigrams[word] += 1\n        \ndf_toxic_bigrams = pd.DataFrame(sorted(toxic_bigrams.items(), key=lambda x: x[1])[::-1])\n\nbigrams_100 = df_toxic_bigrams[:N]\n","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:23.162376Z","iopub.execute_input":"2022-02-03T19:38:23.162595Z","iopub.status.idle":"2022-02-03T19:38:37.392063Z","shell.execute_reply.started":"2022-02-03T19:38:23.16257Z","shell.execute_reply":"2022-02-03T19:38:37.391221Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"fig, axes = plt.subplots(ncols=2, figsize=(18, N//2), dpi=100)\nplt.tight_layout()\n\nsns.barplot(y=unigrams_100[0], x=unigrams_100[1], ax=axes[0], color='#4dabf8')\nsns.barplot(y=bigrams_100[0], x=bigrams_100[1], ax=axes[1], color='#ae91f4')\n\nfor i in range(2):\n    axes[i].set_xlabel('')\n    axes[i].set_ylabel('')\n    axes[i].tick_params(axis='x', labelsize=13)\n    axes[i].tick_params(axis='y', labelsize=13)\n\naxes[0].set_title(f'Top {N} most common unigrams in comments text', fontsize=15)\naxes[1].set_title(f'Top {N} most common bigrams in comments text', fontsize=15)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:37.393492Z","iopub.execute_input":"2022-02-03T19:38:37.393811Z","iopub.status.idle":"2022-02-03T19:38:38.174313Z","shell.execute_reply.started":"2022-02-03T19:38:37.393771Z","shell.execute_reply":"2022-02-03T19:38:38.173371Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h2 id=\"heading\">\n<div style=\"color:white;\n           display:fill;\n           border-radius:5px;\n           background-color:#5642C5;\n           font-size:110%;\n           font-family:Verdana;\n           letter-spacing:0.5px\">\n\n<p style=\"padding: 5px;\n              color:white;\">\n           WordCloud\n</p>\n</div>\n</h2>\n","metadata":{}},{"cell_type":"markdown","source":"We may pay little attention to what we say on a daily basis, if we visualize the toxic comments, \n\nwould you want these comments come from your mouth üíã,\n\nfill your heart ‚ô•Ô∏è , \n\nand grow as a tree üå¥in your mind ? \n\nOr would you and you loved ones be comfortable to surrounded by those toxicated words?","metadata":{}},{"cell_type":"code","source":"# Generate a word cloud image with the shape of toxic \ndef wcloud(data,var, path):\n    mask = np.array(Image.open(path))\n    text_mt = \" \".join(t for t in data[var])\n    wordcloud_usa = WordCloud(stopwords=STOPWORDS, background_color=\"white\", max_words=1000, mask=mask).generate(text_mt)\n\n    # create coloring from image\n    image_colors = ImageColorGenerator(mask)\n    plt.figure(figsize=[10,10])\n    plt.imshow(wordcloud_usa.recolor(color_func=image_colors), interpolation=\"bilinear\")\n    plt.axis(\"off\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:38.175517Z","iopub.execute_input":"2022-02-03T19:38:38.175752Z","iopub.status.idle":"2022-02-03T19:38:38.183271Z","shell.execute_reply.started":"2022-02-03T19:38:38.175718Z","shell.execute_reply":"2022-02-03T19:38:38.182233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wcloud(val,\"more_toxic\", \"../input/picture1/m1.png\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:38.184356Z","iopub.execute_input":"2022-02-03T19:38:38.184568Z","iopub.status.idle":"2022-02-03T19:38:52.685104Z","shell.execute_reply.started":"2022-02-03T19:38:38.184545Z","shell.execute_reply":"2022-02-03T19:38:52.684216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"wcloud(val,\"less_toxic\", \"../input/picture2/h1.png\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:38:52.686859Z","iopub.execute_input":"2022-02-03T19:38:52.687202Z","iopub.status.idle":"2022-02-03T19:39:11.902039Z","shell.execute_reply.started":"2022-02-03T19:38:52.68716Z","shell.execute_reply":"2022-02-03T19:39:11.900326Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#comments without sentimental approaches\nwcloud(df_train,\"comment_text\", \"../input/picture5/t2.png\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:39:11.903287Z","iopub.execute_input":"2022-02-03T19:39:11.903504Z","iopub.status.idle":"2022-02-03T19:40:27.168263Z","shell.execute_reply.started":"2022-02-03T19:39:11.903479Z","shell.execute_reply":"2022-02-03T19:40:27.167414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"heading\">\n5. üîÆ TF -IDF: Term Frequency Technique\n<a class=\"anchor-link\" href=\"https://www.kaggle.com/fangya/jigsaw-mindful-words/edit/run/84913255\">¬∂</a>\n</h1>\n\n**TF-IDF**: Term Frequency -Inverse Dense Frequency is the most useful and straigtforward technique for machine to understand the importance and relevance of words to their frequncies in sentences or documents.\n\n#### TF IDF Applications:\n\n1. **Keyword Extraction**\nHighest scoring words\n\n2. **Information Retrieval**\nSearch engine display in order of relevance for the Keyword\n\n#### Formula:\n\n$$TF(Term Frequency)= \\frac{\\# \\ of \\ repetition \\ of \\ word \\ in \\ sentence}{Total \\# \\ words \\ in \\ sentence}$$\n\n$$IDF (Inverse Dense Frequency) = log \\frac{\\# of \\ sentences}{\\# \\ of \\ sentences \\ containing \\ the \\ word}$$\n\n$$Score \\ Matrix = TF* IDF $$\n\nthe closer the score is to 0 , the more common the word is\n","metadata":{}},{"cell_type":"code","source":"# Create a score that messure how much toxic is a comment\nrandom_score = {'obscene': 0.20, 'toxic': 0.40, 'threat': 0.6, \n            'insult': 0.65, 'severe_toxic': 0.9, 'identity_hate': 0.9}\n\nfor category in random_score:\n    df_train[category] = df_train[category] * random_score[category]\n\ndf_train['score'] = df_train.loc[:, 'toxic':'identity_hate'].mean(axis=1)\ndf_train['y'] = df_train['score']\n\nmin_len = (df_train['y'] > 0).sum()  # len of toxic comments\ndf_non_tox = df_train[df_train['y'] == 0].sample(n=min_len, random_state=201)  # take non toxic comments\ndf_train_new = pd.concat([df_train[df_train['y'] > 0], df_non_tox])  # make new df\ndf_train_new.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:47:47.020261Z","iopub.execute_input":"2022-02-03T19:47:47.021306Z","iopub.status.idle":"2022-02-03T19:47:47.131227Z","shell.execute_reply.started":"2022-02-03T19:47:47.021259Z","shell.execute_reply":"2022-02-03T19:47:47.130222Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_samples_toxic = len(df_train[df_train['score'] != 0])\nn_samples_normal = len(df_train) - n_samples_toxic\n\nidx_to_drop = df_train[df_train['score'] == 0].index[n_samples_toxic//5:]\ndf_train = df_train.drop(idx_to_drop)\n\nprint(f'Reduced number of neutral text samples from {n_samples_normal} to {n_samples_toxic//5}.')\nprint(f'Total number of training samples: {len(df_train)}')","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:47:53.273917Z","iopub.execute_input":"2022-02-03T19:47:53.274386Z","iopub.status.idle":"2022-02-03T19:47:53.362782Z","shell.execute_reply.started":"2022-02-03T19:47:53.27435Z","shell.execute_reply":"2022-02-03T19:47:53.361687Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_tragets = pd.DataFrame(pd.unique(df_train['score'].values), columns=['target_value']).sort_values(by='target_value', ascending = True).reset_index(drop=True)\nTHRESHOLD = df_tragets['target_value'].quantile(q=0.2)\ndf_train['sentiment'] = df_train['score'].map(lambda x: 1 if x < THRESHOLD else 2 )\n\ndf_train = df_train[['comment_text','sentiment']].reset_index(drop=True)\ndf_train","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:48:02.684503Z","iopub.execute_input":"2022-02-03T19:48:02.684779Z","iopub.status.idle":"2022-02-03T19:48:02.724547Z","shell.execute_reply.started":"2022-02-03T19:48:02.684745Z","shell.execute_reply":"2022-02-03T19:48:02.723566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(analyzer='word',stop_words= 'english')\nX = tf_idf_vect.fit_transform(df_train['comment_text']).toarray()\nX\ndf_test = pd.read_csv(comments_to_score_path)\n\n#Clean step 1, 2 and 3\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in clean_text(x) if w not in unrelevant_words]))\n\n#Clean Step 4\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in stopwords(x,stop_words)]))\n\n#Clean Step 5\ndf_test['text'] = df_test['text'].apply(lambda x: ''.join([w for w in lemmatizer.lemmatize(x)]))\n\ndf_test.head(3)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:48:32.484049Z","iopub.execute_input":"2022-02-03T19:48:32.484425Z","iopub.status.idle":"2022-02-03T19:48:42.178033Z","shell.execute_reply.started":"2022-02-03T19:48:32.484391Z","shell.execute_reply":"2022-02-03T19:48:42.177216Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tf_idf_vect = TfidfVectorizer(analyzer='word',stop_words= 'english')\nY = tf_idf_vect.fit_transform(df_test['text']).toarray()\nY","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:48:46.012365Z","iopub.execute_input":"2022-02-03T19:48:46.013272Z","iopub.status.idle":"2022-02-03T19:48:47.290123Z","shell.execute_reply.started":"2022-02-03T19:48:46.013186Z","shell.execute_reply":"2022-02-03T19:48:47.289278Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"heading\">\n6. üìä Modeling \n<a class=\"anchor-link\" href=\"http://localhost:8888/notebooks/HU/3.Kaggle/Jigsaw.ipynb\">¬∂</a>\n</h1>\n\nWe will compare the LinearSVC , Logistic Regression, Naive Bayesians, and Randomforest models using the balanced data.","metadata":{}},{"cell_type":"code","source":"#clean text\nclean(df_train1,\"comment_text\")","metadata":{"execution":{"iopub.status.busy":"2022-02-03T20:10:34.621385Z","iopub.execute_input":"2022-02-03T20:10:34.621745Z","iopub.status.idle":"2022-02-03T20:10:34.71658Z","shell.execute_reply.started":"2022-02-03T20:10:34.621655Z","shell.execute_reply":"2022-02-03T20:10:34.71547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for col in ['toxic','severe_toxic','obscene','threat','insult','identity_hate']:\n#     print(f'------------------------{col}-----------------------')\n    print(col.center(40, '.'))\n    display(df_train1.loc[df_train1[col]==1,['comment_text',col]].sample(2))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.616345Z","iopub.status.idle":"2022-02-03T19:40:27.61673Z","shell.execute_reply.started":"2022-02-03T19:40:27.616553Z","shell.execute_reply":"2022-02-03T19:40:27.616573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_train1['y'] = (df_train1[['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].sum(axis=1) > 0 ).astype(int)\ndf_train_binary = df_train1[['comment_text', 'y']].rename(columns={'comment_text': 'text'})\ndf_train_binary.sample(5)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.61798Z","iopub.status.idle":"2022-02-03T19:40:27.618328Z","shell.execute_reply.started":"2022-02-03T19:40:27.618151Z","shell.execute_reply":"2022-02-03T19:40:27.618173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display(df_train_binary.loc[df_train_binary['y']==1,['text','y']].sample(5))\ndisplay(df_train_binary.loc[df_train_binary['y']==0,['text','y']].sample(5))","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.619759Z","iopub.status.idle":"2022-02-03T19:40:27.620307Z","shell.execute_reply.started":"2022-02-03T19:40:27.620063Z","shell.execute_reply":"2022-02-03T19:40:27.620092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Unbalanced vs Balanced Dataset\n\nThere are more records of y=0, therefore we will balance the record number.","metadata":{}},{"cell_type":"code","source":"df_lt=df_train_binary.loc[df_train_binary['y']==0]\ndf_train_binary['y'].value_counts(normalize=True)\n","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.621207Z","iopub.status.idle":"2022-02-03T19:40:27.622059Z","shell.execute_reply.started":"2022-02-03T19:40:27.621856Z","shell.execute_reply":"2022-02-03T19:40:27.621887Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"toxic_len = (df_train_binary['y'] == 1).sum()\nprint(toxic_len)\ndf_train_balanced = df_train_binary[df_train_binary['y'] == 0].sample(n=toxic_len)\ndf_train_balanced['y'].value_counts(normalize=True)\ndf_train_b = pd.concat([df_train_binary[df_train_binary['y'] == 1], df_train_balanced])\ndf_train_b['y'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.623292Z","iopub.status.idle":"2022-02-03T19:40:27.623807Z","shell.execute_reply.started":"2022-02-03T19:40:27.623626Z","shell.execute_reply":"2022-02-03T19:40:27.623644Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tfidf = TfidfVectorizer(sublinear_tf=True, min_df=5, norm='l2', encoding='latin-1', ngram_range=(1, 2), stop_words='english')\nfeatures = tfidf.fit_transform(df_train_b['text']).toarray()\nlabels = df_train_b['y']\nfeatures.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.624749Z","iopub.status.idle":"2022-02-03T19:40:27.625263Z","shell.execute_reply.started":"2022-02-03T19:40:27.625036Z","shell.execute_reply":"2022-02-03T19:40:27.625063Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Model comparison\n\nAs we can see , LinearSVC and Logistic Regression, Naive Bayesian has great performance level, even RF is relatively good.\n\n**cross_val_score() Parameters:**\n\n- estimator: estimator object implementing ‚Äòfit‚Äô The object to use to fit the data.\n\n- X: array-like of shape (n_samples, n_features) The data to fit. Can be for example a list, or an array.\n\n- y: array-like of shape (n_samples,) or (n_samples, n_outputs), default=None The target variable to try to predict in the case of supervised learning.\n\n- groups: array-like of shape (n_samples,), default=None Group labels for the samples used while splitting the dataset into train/test set. Only used in conjunction with a ‚ÄúGroup‚Äù cv instance (e.g., GroupKFold).\n\n- scoring: str or callable, default=None A str (see model evaluation documentation) or a scorer callable object / function with signature scorer(estimator, X, y) which should return only a single value.\n\n- cv: int, cross-validation generator or an iterable, default=None Determines the cross-validation splitting strategy. Possible inputs for cv are:\n\nNone, to use the default 5-fold cross validation,\n\n**Returns:**\n\nscores: ndarray of float of shape=(len(list(cv)),) Array of scores of the estimator for each run of the cross validation.\n\n","metadata":{}},{"cell_type":"code","source":"models = [\n    RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0),\n    LinearSVC(),\n    MultinomialNB(),\n    LogisticRegression(random_state=0),\n]\nCV = 5\ncv_df = pd.DataFrame(index=range(CV * len(models)))\nentries = []\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, features, labels, scoring='accuracy', cv=CV)\n    for fold_idx, accuracy in enumerate(accuracies):\n        entries.append((model_name, fold_idx, accuracy))\ncv_df = pd.DataFrame(entries, columns=['model_name', 'fold_idx', 'accuracy'])\n\nsns.boxplot(x='model_name', y='accuracy', data=cv_df)\nsns.stripplot(x='model_name', y='accuracy', data=cv_df, \n              size=8, jitter=True, edgecolor=\"gray\", linewidth=2)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.626214Z","iopub.status.idle":"2022-02-03T19:40:27.626711Z","shell.execute_reply.started":"2022-02-03T19:40:27.626483Z","shell.execute_reply":"2022-02-03T19:40:27.626509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cv_df.groupby('model_name').accuracy.mean()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.627554Z","iopub.status.idle":"2022-02-03T19:40:27.628223Z","shell.execute_reply.started":"2022-02-03T19:40:27.627989Z","shell.execute_reply":"2022-02-03T19:40:27.628013Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Confusion Matrix\nIf we use Linear SVC, the accuracy is over 90% !","metadata":{}},{"cell_type":"code","source":"from sklearn.calibration import CalibratedClassifierCV\nsvm = LinearSVC()\nclf = CalibratedClassifierCV(svm) \nclf.fit(features, labels)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.629364Z","iopub.status.idle":"2022-02-03T19:40:27.629685Z","shell.execute_reply.started":"2022-02-03T19:40:27.629517Z","shell.execute_reply":"2022-02-03T19:40:27.629538Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(features)\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(labels, y_pred)\nfig, ax = plt.subplots(figsize=(10,10))\ncmap = \"tab20\"\nsns.heatmap(conf_mat, annot=True, fmt='d',cmap=cmap)\nplt.ylabel('Actual')\nplt.xlabel('Predicted')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.631013Z","iopub.status.idle":"2022-02-03T19:40:27.63171Z","shell.execute_reply.started":"2022-02-03T19:40:27.631501Z","shell.execute_reply":"2022-02-03T19:40:27.631529Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"<h1 id=\"heading\">\n7. üíØ Submission\n<a class=\"anchor-link\" href=\"http://localhost:8888/notebooks/HU/3.Kaggle/Jigsaw.ipynb\">¬∂</a>\n</h1>","metadata":{}},{"cell_type":"code","source":"# clean test text\nclean(df_sub, \"text\")\ndf_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.632736Z","iopub.status.idle":"2022-02-03T19:40:27.633535Z","shell.execute_reply.started":"2022-02-03T19:40:27.633328Z","shell.execute_reply":"2022-02-03T19:40:27.633353Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_sub = df_sub['text']\nX_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.634513Z","iopub.status.idle":"2022-02-03T19:40:27.63484Z","shell.execute_reply.started":"2022-02-03T19:40:27.634674Z","shell.execute_reply":"2022-02-03T19:40:27.634694Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test= tfidf.transform(df_sub['text'])\nX_test.shape","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.636168Z","iopub.status.idle":"2022-02-03T19:40:27.636742Z","shell.execute_reply.started":"2022-02-03T19:40:27.636537Z","shell.execute_reply":"2022-02-03T19:40:27.63656Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_test = clf.predict_proba(X_test)\nlen(y_test)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.637686Z","iopub.status.idle":"2022-02-03T19:40:27.638013Z","shell.execute_reply.started":"2022-02-03T19:40:27.637839Z","shell.execute_reply":"2022-02-03T19:40:27.637858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub['score'] = y_test[:, 1]\ndf_sub['score'].head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.639969Z","iopub.status.idle":"2022-02-03T19:40:27.640523Z","shell.execute_reply.started":"2022-02-03T19:40:27.640346Z","shell.execute_reply":"2022-02-03T19:40:27.640363Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.head()","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.641218Z","iopub.status.idle":"2022-02-03T19:40:27.641887Z","shell.execute_reply.started":"2022-02-03T19:40:27.641698Z","shell.execute_reply":"2022-02-03T19:40:27.641716Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub[['comment_id', 'score']].to_csv(\"submission.csv\", index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-03T19:40:27.64278Z","iopub.status.idle":"2022-02-03T19:40:27.644992Z","shell.execute_reply.started":"2022-02-03T19:40:27.644804Z","shell.execute_reply":"2022-02-03T19:40:27.644824Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Summary\n\nThere's definitely improvement we can make for this project, such as the text cleanning, words like \"Wiki\", \"people\" are still presented. We tried to run the LSMT model, but the Kernel collapsed, @jessie ran a whole afternoon. While combining Eileen and Kathleen's work, my Notebook collapesed many times due to lack of memory(*your notebook tried to allocate more memory than its availble*). Despite all these issues, I think we learned many things along the way and did a great job!\n\nThanks @Kathleen Lee and @Eileen_ds for trying different modeling, if you are interested to see the results, please click the reference link below.\n\nAlthough it's important we remind ourselves use polite and gentle words to others, I also think it is crucial to stand up for ourselves, if someone is mean and malicious, \n**Don't be afraid, Don't be shy , Fight Back!**\n\n>‚Äú‰∏∫‰ªÄ‰πàÊâÄË∞ìÊïôÂÖªÂ∞±ÊòØÂèóËã¶ÁöÑ‰∫∫ËØ•Èó≠Âò¥Ôºü‚Äù‚Äî‚Äî„ÄäÊàøÊÄùÁê™ÁöÑÂàùÊÅã‰πêÂõ≠„Äã\n\n\n** ‚≠êÔ∏è Happy Coding ! Thanks for Reading üòù **\n","metadata":{}},{"cell_type":"markdown","source":"<h1 id=\"heading\">\n8. üìô Reference\n<a class=\"anchor-link\" href=\"http://localhost:8888/notebooks/HU/3.Kaggle/Jigsaw.ipynb\">¬∂</a>\n</h1>\n\nhttps://medium.com/analytics-vidhya/text-cleaning-in-natural-language-processing-nlp-bea2c27035a6\n\nhttps://www.kaggle.com/kathleenlee/jigsaw\n\nhttps://www.kaggle.com/eileends/jigsaw-model-selection\n\nhttps://medium.datadriveninvestor.com/tf-idf-in-natural-language-processing-8db8ef4a7736\n\nhttps://www.kaggle.com/kishalmandal/most-detailed-eda-tf-idf-and-logistic-reg\n\nhttps://www.kaggle.com/kl299792458/jigsaw4-kehan-liu\n\nhttps://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f","metadata":{}}]}